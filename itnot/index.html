<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				
<!––  Slide0 ––>				
				<section><p><b>In the NIC of time</b></p>
				<p>Enabling high-performance edge applications with OpenStack, OVS, and SmartNICs<p/>
                                    <aside class="notes">
                                        Thank you, Daniel, for the (nice? warm?) introduction.
                                    </aside>
				</section>
<!––  Slide1 ––>
				<section>
			        <img src="pics/slide1.png" width=70% height=70%></th>
				    <aside class="notes">
<p>So! Hello everyone, my name is Elena "I won't pronounce" Lindqvist and I am here to tell you about using smartNICs with OpenStack.</p>
<p>I work at Ericsson as a systems manager. That's *systems* manager, so ... </p>
				    </aside>
				</section>
<!––  Slide2 ––>				
				<section>
			        <img src="pics/slide2_meeting.jpg" width=70% height=70%></th>
				    <aside class="notes">
... not this ... - advance to next slide - 

				    </aside>
				</section>
<!––  Slide3 ––>	
				<section>
			        <img src="pics/slide3_HDS.jpg" width=70% height=70%></th>
					<aside class="notes">
but this ... - advance to next slide - 
					</aside>
				</section>
<!––  Slide4 ––>	
				<section>
			        <img src="pics/slide3_datacenter.jpg" width=70% height=70%></th>
					<aside class="notes">
or this ... - advance to next slide - 
					</aside>
				</section>		
<!––  Slide5 ––>			
				<section>
					<tr>
					    <th><img src="pics/slide5_erlang.jpg" width=44% height=44%></th> 
					    <th><img src="pics/slide5_bluetooth.png" width=25% height=25%></th>
					</tr>
					<tr>
					<p>
					    <th>40% of all mobile traffic crunching data</th>	
					</p>
				        </tr>
					<aside class="notes">
If you don't mind, I'd like to tell you a few things about Ericsson. We're NOT making phones! as most of the people think ....
I can tell you that no matter where you are in the world, when you are connected to a mobile network and you access the internet, there is a high chance that your traffic goes through our stuff, radio base stations, servers running ericsson software etc...
40% of all mobile traffic world wide goes through our stuff.

And since we are at an Openstack meetup and talking about open source, it is probably good to mention that Ericsson has given to open source Erlang (Ericsson Language, or named after a Danish mathematician, whichever you prefer)
In Openstack, RabbitMQ uses Erlang.
Bluetooth comes from Ericsson too ...
					</aside>
				</section>
<!––  Slide6 ––>	
				<section>
					<tr>
						<th><img src="pics/slide6_openstack-logo.png" width=34% height=34%></th> 
					</tr>
					<tr>
					    <th><img src="pics/slide6_nfv.jpeg" width=50% height=50%></th>
					</tr>

					<aside class="notes">
At Ericsson, we use OpenStack for virtual infrastructure management, as part of our NFVi solution. 
Our customers are typically telecom operators ... well, and a taxi company ... in Dubai ... and Panasonic Avionics for entertainment on board of flights, and many other such examples. 
What is NFVi? 
For many years at Ericsson, we made SW and HW very dependent on each other. Customers bought whole racks of custom HW and the Ericsson SW applications deployed and running on top of that HW.
NFVi is part of the NFV framework and it means, more or less, decoupling the SW from HW for network nodes using virtualization. It means you can run the telecom applications(the SW) on any HW (like Dell, HP, Qanta, SuperMicro, Fujitsu servers, whatnot), in VMs or containers.
Traces of this decoupling of the network functions from proprietary hardware appliances are there for many years now. 
Around the year 2003, I worked in an ISP. We used Cisco routers to do BGP with customers and the upstream provider. I was in awe when GNU Zebra came out and I could run BGP in a Linux box. Fast forward to today, part of SDN, we use opendaylight with Quagga soft router for BGP ( Quagga is what followed after zebra, it is actually an extinct sub-specie of the African zebra.)


- advance to next slide - 
					</aside>
				</section>
<!––  SlideX ––>
                                <section> 									
<p><b>88%</b> increase in mobile data traffic  Q4-2017 to Q4-2018</p>
<p><b>88%</b> increase in mobile data traffic  Q4-2017 to Q4-2018</p>
<p><b>EPC</b> to the rescue</p>										 
                                        <aside class="notes">                                      

Some applications require high throughput and Ericsson's EPC is one notable example.
EPC is the equivalent of formerly used GPRS, it is there to make mobile data traffic possible. It means you traverse it when surfing the internet from your mobile, or when watching youtube, Netflix, GoT, playing Pokemon?
According to Ericsson Mobility Report, the monthly mobile data traffic grew close to 88% between Q4 2017 and Q4 2018, this is mainly due to the increased traffic per smartphone in China. 
According to the same report, mobile traffic is 50% video today and it will increase to 75% video in 2020, driven by, amongst others, AR/VR applications. 

This increase in number of devices using the mobile network and the traffic generated by them, it boils down to EPC will need to handle all this traffic.
How to cope with this in a performant way, well, maybe smartNICs could have the answer.  
gtp tunnel to EPC VMs , descapsulate traffic inside the VM                                       
								</aside>
                                </section>

<!––  Slide7 ––>	
				<section>
					<tr>
					    <th><img src="pics/slide7_smartNIC.png" width=100% height=100%></th>
					</tr>
					<aside class="notes">
This is how a smartNIC looks like !

To cope with this increase in traffic, we are looking at using smartNICs in Openstack on top of which we run vEPC in VMs. 
What is a smartNIC?
A smartNIC is a NIC capable of offloading processing from the host CPU. To be able to do this, it can have its own CPU, memory and flash storage embedded on the card. 
To offload processing from CPU, a smartNIC will, for instance, run the ovs control and data plane, a firewall or performance acceleration techniques, *inside* the NIC. (That is, you will run your ovs-vsctl show commands while logged in to the NIC). - advance to next slide - 
					</aside>
				</section>				
<!––  Slide8 ––>
                                <section>
                                        <tr>
                                            <th><img src="pics/slide8_intel.png" width=24% height=24%></th>
                                            <th><img src="pics/slide8_mellanox.jpg" width=45% height=45%></th>
                                        </tr>
                                        <tr>
                                            <th><img src="pics/slide8_Broadcom.jpg" width=44% height=44%></th>
                                            <th><img src="pics/slide8_netronome.png" width=45% height=45%></th>
                                        </tr>                                        
                                        <aside class="notes">
There are quite a few types of smartNICs, ranging from thousands to a few hundred dollars(some you can buy on eBay).

Some smartNICs use an FPGA (Field Programmable Gate Array) mounted on the PCIe network card. Using FPGA boards provides flexibility, they can be easily programmed and updated once installed. (P4-Programmable Protocol-independent Packet Processor- can be used for programming packet forwarding planes.)

Some smartNICs do not use an FPGA, and are ASIC based instead, they are less flexible but still capable of doing lots of things, at a significantly lower price. 

SmartNICs might use ARM CPUs in a RISC architecure or x86 CPUs, different amounts of RAM, they can consume more or less power.
SmartNICs can have different form factors HHHL(half hight half lenght) if you need to fit it in a 2Us compute  or FHFL full hight full lenght for wider than 2Us computes.

You can opt for 2 or 4 X 25, 4X 50, 2 X 100 Gbps ports, that is two ports (cages) on a NIC wifh HHHL form factor and  4 ports on NICs with FHFL form factor. 
You have options around the number of PCI lanes to be used, x8 or x16.  - advance to next slide - 
                                        </aside>
                                </section>

<!––  Slide9 ––>
                                <section>
									                                        <tr>

					 <p>Is it a bird? is it a plane?</p>
					 <p> It's SmartNIC !</p>
					                                         </tr>

                                        <aside class="notes">
Is it a bird? is it a plane? It's SmartNIC !

How should your architecture look like when using smartNICs.

You could use one smartNIC for data per compute, we're tal
I was able to reach the new vnic from one of the computes.
This is what I did.

On leaf-1-a
-----------
vlan-create id 400 scope fabric
switch-vnic-create ip 192.168.77.2/24 vlan 400 if mgmt
admin-service-modify if eth1.400 web-ssl

On leaf-1-b
-----------
switch-vnic-create ip 192.168.77.3/24 vlan 400 if mgmt
admin-service-modify if eth1.400 web-ssl

On compute-1-2
--------------
vconfig add bond1 400
ifconfig bond1.400 192.168.77.11/24

I dont know how to propagate this connection into the vCIC. I need some help from CEE with this.
This at least shows it is possible to set up. CEE king 2x100 Gbps here, that's *a lot* of bandwidth.
In this case, high availability happens at compute level, not at network card level. 

What if you have a dual socket system. If you plug a smartNIC in a PCIe socket, applications might not like crossing that QPI link (from 2017 called UPI) between the CPUs.
In this case you can look into using a bifurcated smartNIC, that's basically splitting the card in two physical pieces that you can insert in two PCIe slots, one per NUMA node.

If you use two smartNICs, do you want two separate ovs controllers in your compute? will OpenStack even support that? 

                                        </aside>
                                </section>
                                
<!––  Slide9 ––>
                                <section>
                                        <tr>
					 <p>What is this smartNIC, anyway?</p>
					 <p>Is it an embedded linux, is it a linux mini server?</p>
					 
                                        </tr>
                                        <aside class="notes">

What is this smartNIC, anyway? Is it an embedded linux, is it a linux mini server?
Would you want the possibility to say, in one go, configure all your smartNICs to PXE boot, so you can load a new linux on them. Then you need something like IPMI, we're talking rather something like a linux server here than an embedded linux. 

What security concerns are raised with introducing a smartNIC with linux running on it. I'm looking at you, Kim!

                                        </aside>
                                </section>                                
<!––  Slide9 ––>
                                <section>
                                        <tr>
					 <p>When should you use a smartNIC?</p>
					 <p>Do you use more than 4CPUs for OVS per compute host?</p>
					 
                                        </tr>
                                        <aside class="notes">

When should you use a smartNIC?
(Intel info) If on your host you use more than 4CPUs for OVS, then you should switch to using smartNICs, it makes sense from a business point of view. 
(Also, smartNIC is a good idea if you need low latency and don't care so much about migration)
- advance to next slide - 
                                        </aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>Picture ovs CPU allocation, cpuisol </p>                                  
                                        <aside class="notes">                                      
Here you can see the CPUs allocated to ovs, in my case X number of CPUs, the one in cpuisol are the CPUs that will be used by nova
the ones missing are two? for host OS and rest for OVS
in yaml file also 
								</aside>
                                </section>
                                                                                                
<!––  Slide10 ––>
                                <section>   <p>Ironic Neutron Cyborg</p>
                                        <tr>
                                            <th><img src="pics/OpenStack_Project_Ironic_mascot.png" width=25% height=25%></th>
                                            <th><img src="pics/OpenStack_Project_Neutron_mascot.png" width=25% height=25%></th>
                                            <th><img src="pics/OpenStack_Project_Cyborg.png" width=25% height=25%></th>
                                        </tr>
                                                                        
                                        <aside class="notes">
											Openstack working with SmartNICs 
Now the question is where are we in Openstack when it comes to integrating the wide range of smartNICs appearing on the market?
Work is done in several Openstack projects, like ironic, nova and neutron of course.	

For instance when it comes to neutron, we need changes in the Neutron OVS driver and Neutron OVS agent in order to bind the Neutron port for the baremetal host with the smartNIC.
This is needed so that neutron ovs agent can configure the OVS running on the smartNIC.

We can have neutron ovs agent running locally on the smartNIC or remotely and manages the OVS bridges for all baremetal smartNICs.

There are many interesting questions raised, like how do you know which smartNIC hostname belongs to which server.
Ovs 

Ironic
Is Openstack ready to use smartNICs?
In Openstack we need to manage the smartNICs that are running full(y fledged) operating system inside the card.

Nova: 
The smartNICs can do packet processing inside the NIC so in order to support this hw acceleration, nova needs some changes. 

Openstack Cybborg

Some smartNIC vendors might use native virtio driver while others use proprietary drivers.
	
https://specs.openstack.org/openstack/nova-specs/specs/pike/implemented/netronome-smartnic-enablement.html
									
Openstack working with SmartNICs - TO BE ADDED
Openstack support for smartNICs starting with Rocky, Stein - TO BE ADDED
Neutron integration with SmartNICs - TO BE ADDED
Openstack Cyborg for integrating FPGAs with openstack - TO BE ADDED

- advance to next slide - 
                                        </aside>
                                </section>

<!––  SlideX ––>
                                <section>
<font size="-2">										
<p style="text-align:left;">root@cic-1:~# openstack  port create<br>                                        
<p style="text-align:left;">usage: openstack port create [-h] [-f {json,shell,table,value,yaml}]<br>                                        
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-c COLUMN] [--max-width <integer>] [--fit-width]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--print-empty] [--noindent] [--prefix PREFIX]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--network <network> [--description <description>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--device <device-id>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--mac-address <mac-address>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--device-owner <device-owner>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--vnic-type <vnic-type>] [--host <host-id>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--dns-name dns-name]</p>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--fixed-ip subnet=<subnet>,ip-address=<ip-address>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--binding-profile <binding-profile>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--enable | --disable] [--project <project>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--project-domain <project-domain>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--security-group <security-group> | --no-security-group]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--qos-policy <qos-policy>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--enable-port-security | --disable-port-security]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--allowed-address ip-address=<ip-address>[,mac-address=<mac-address>]]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--tag <tag> | --no-tag]<br>
<p style="text-align:left;">--vnic-type <vnic-type><br>
<p style="text-align:left;">VNIC type for this port (direct | direct-physical | macvtap | normal | baremetal, default: normal)</p>                         
                    
</font>									 
                                        <aside class="notes">                                      
In ironic
Steps: 
(https://specs.openstack.org/openstack/neutron-specs/specs/stein/neutron-ovs-agent-support-baremetal-with-smart-nic.html)
1. Create Neutron port with smart-nic vnic_type , this is done in Neutron OVS ML2 driver 
2. local_link_information wiht info like smartNIC hostname , port ID, ssh public key, ovsdb ssl certificate                                    
								</aside>
                                </section>                                

<!––  SlideX ––>
                                <section>
<font size="-0.5">					 
<p style="text-align:left;">root@FPA1066GX-DA2:~# ovs-appctl  dpif/dump-flows br0</p>  
<p style="text-align:left;">in_port(14),eth(src=68:05:ca:91:36:b5,dst=68:05:ca:91:36:b5),eth_type(0x0800),ipv4(src=192.168.0.1,dst=192.168.1.1,proto=6),</p>                                        
<p style="text-align:left;">tcp(src=1234,dst=5678), packets:0, bytes:0, used:7.340s, actions:<b><font color="red">drop</font></b></p>  
</font>	                                                                       									
                                        <aside class="notes">                                      
<p>Yeah, I chucked this in just as an example of what it means to play with non-commercial NICs, can you spot the problem?</p>                                        
								</aside>
                                </section>
                                                                

<!––  Slide11 ––>
                                <section> 
<p>Performance</p>
<p>Kernel Space vs User Space</p>

                                        <aside class="notes">
											

If you care about latency and packet processing performance there are a few options. 

We basically need to overcome the limitations in the Linux kernel which is not ideal for *lots* of packet-processing.

Why is the Linux kernel a problem when we talk about latency and performance ( with performance, I mean higher throughput at a lower CPU cost)? 
Well, the Linux kernel is monolithic, it's millions of lines of code.
It contains lots of things like drivers (which makes the Linux kernel work with any hw, not just your specific hw/smartNIC), it allows running many applications at the same time by using a time sharing layer. Resources like CPU, mem exposed by the kernel are shared between all the processes running.

The networking stack inside the Linux kernel limits how many packets per second it can process, it was conceived to be slow, a decision taken 20 25 years ago, that packets should be delivered into sockets. If you want to be fast you dont do this upfront. 
Too many packets per second means CPUs get busy just receiving packets, then either the packets are dropped or we CPU starve the applications.



<!––  TAKE IT OUT this paragraph or rephrase it ––>
How your operating system deals with data
Data destined to a particular system is first received by the NIC and is stored in the ring buffer of the reception (RX) present in the NIC, which also has TX (for data transmission). Once the packet is accessible to the kernel, the device driver raises softirq (software interrupt), which makes the DMA (data memory access) of the system send that packet to the Linux kernel. The packet data in the Linux kernel gets stored in the sk_buff data structure to hold the packet up to MTU (maximum transfer unit). When all the packets are filled in the kernel buffer, they get sent to the upper processing layer – IP, TCP or UDP. The data then gets copied to the preferred data receiving process.
Note: Initially, a hard interrupt is raised by the device driver to send data to the kernel, but since this is an expensive task, it is replaced by a software interrupt. This is handled by the NAPI (new API), which makes the processing of incoming packets more efficient by putting the device driver in polling mode.
                                        </aside>
                                </section>


<!––  SlideX ––>
                                <section> 
<p>Kernel bypass with DPDK</p>                                  
                                        <aside class="notes">
To get better performance, one can choose to bypass the kernel, fully or partially.
There are several kernel bypass options like: 
DPDK, (that would be the poster child of kernel bypass)
Snabbswitch, 
PF_RING, 
Netmap. (I am personally more familiar with DPDK.)


With kernel bypass, we move the NIC to the user-space.
If the NIC is managed in the user-space, it means we skip things like context switching, networking layer processing, interrupts that happen in the kernel aka IRQ storms and do the packet-processing in the user-space.
This is relevant at 10Gbps already. EPC today saturates 200Gbps already. 

NUMA awareness together with CPU isolation needs to be considered as well if we need high performance. 
Remember this is particularly interesting when using smartNICs, with a dual socket system using one smartNIC plugged in on PCIe slot, corresponding to one NUMA socket.

Moving to userspace means losing the abstraction level the kernel provides for e.g. hw resources, it means you need to load own driver.
Moving to userspace means the kernel space is skipped together with the good stuff too like networking functionality that needs to be reimplemented now. (like the whole TCP stack)										                                      
                                       
								</aside>
                                </section>

<!––  SlideX ––>
                                <section> 

<font size="-2">										
<p style="text-align:left;">$ echo 0000:18:00.4 > /sys/bus/pci/devices/0000\:18\:00.4/driver/unbind<br>                                        
<p style="text-align:left;">$ echo 0000:18:00.3 > /sys/bus/pci/devices/0000\:18\:00.3/driver/unbind<br>
<p style="text-align:left;">$ echo 0000:18:00.4 > modprobe vfio<br>                                        
<p style="text-align:left;">$ echo 0000:18:00.3 > modprobe vfio_pci<br>
<br>  
<br>                                            
<p style="text-align:left;">$ mkdir -p /dev/hugepages/<br>
<p style="text-align:left;">$ mount -t  hugetlbfs hugetlbfs /dev/hugepages/<br>
<p style="text-align:left;">$ echo 2048 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages<br>
<br>  
<br>                                            
<p style="text-align:left;">$ modprobe uio<br>
<p style="text-align:left;">$ insmod x86_64-native-linuxapp-gcc/kmod/igb_uio.ko<br>
<p style="text-align:left;">$ dpdk-devbind.py -b igb_uio 18:00.2 18:00.3<br>

              
</font>	
                                  
                                        <aside class="notes">

How do you move a device from kernel space to user space, in case of DPDK?
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>XDP and eBPF</p>                                  
                                        <aside class="notes">  

Another way to achieve high performance would be partially bypassing the Linux kernel, for example using XDP. 
XDP  (eXpress Data Path) is a component in the kernel that can be used for fast packet processing. It is an eBPF (I'll get back to explaining what eBPF is) based high performance data path merged in the Linux kernel.
XDP (eXpress Data Path) is shipped with the kernel since version 4.8 and it is enabled by default, with CONFIG_BPF_SYSCALL.										                                    
                                       
								</aside>
                                </section>
                                
<!––  SlideX ––>
                                <section> 
<font size="-1">									
<p style="text-align:left;">$ grep CONFIG_BPF_SYSCALL /boot/config-4.15.0-46-generic</p>                                                                                                                        
<p style="text-align:left;">CONFIG_BPF_SYSCALL=y</p> 
<br>  
<br>
<p style="text-align:left;">prompt: XDP sockets</p> 
<p style="text-align:left;">type: bool</p> 
<p style="text-align:left;">depends on: CONFIG_BPF_SYSCALL</p> 
<p style="text-align:left;">defined in net/xdp/Kconfig</p> 
<p style="text-align:left;">found in Linux kernels: 4.18–4.20, 5.0–5.1, 5.2-rc+HEAD</p> 
                                                                                                                                                    
</font>	
                                        <aside class="notes"> 										
To check if XDP it is enabled in the kernel, it's as simple as grepping for it in the kernel config file 
The Linux kernel configuration item CONFIG_XDP_SOCKETS:       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
                                        <p>
                                                <img src="pics/kernel-diag-ascii.svg" width=35% height=35%>
                                        </p>                                 
                                        <aside class="notes"> 
PICTURE OF XDP 

So XDP is a hook in the Linux kernel, not a kernel bypass but a bypass of the network stack
XDP operates directly on the packet buffer. (packets are moved into sockets , decision taken 20 years ago, and this is not the fastest way)

DPDK steals the whole NIC, we dont do that with XDP , not taking the whole NIC.
a filter on receive but zero copy to user space 


XDP can be used in two ways:
first mode 
Native mode XDP , a driver hook , before memory allocation , small no of instructions executed before we start processing packets
limited number of drivers that support XDP 

second mode 
generic mode
works on any net device , driver independent , but larger number of instructions executed which mean lower performance than native mode, when it comes to packet processing

dataplane which is inside the kernel 
control plane in user space which is done from eBPF, userspace load eBPF program , everything goes through the BPF-syscall

the Native way is the way to go 
XDP driver hook 
						                                     
                                       
								</aside>
                                </section>                                                                
<!––  SlideX ––>
                                <section> 
<p>eBPF is a superpower</p>                                  
                                        <aside class="notes">                                      
What the heck is eBPF ?
eBPF stands for "enhanced Berkeley Packet Filter" it's a linux kernel technology that is used by e.g. tcpdump and other analysis tools.
eBPF is used to extract millions of metrics from the kernel and applications for troubleshooting purposes, deep monitoring or exploring running software.
eBPF is basically like a superpower.
BPF was initially used for tools like tcpdump but  Alexei Starovoitov introduced eBPF to be used  for things like to NATing, routing, doing what iptables does for example.



Jumping between kernel space and user space cost on performance - TO BE ADDED
Mention eBPF with XDP kernel hook,  DPDK, etc .. TO BE ADDED 
Native VirtIO driver benefits - TO BE ADDED
- advance to next slide - 	    			                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
                                 


<!––  Slide12 ––>
                                <section> smartNICs with Storage
                                        <aside class="notes">
I talked mostly about the data traffic so far and using smartNICs with the data traffic.
smartNICs make sense in a lot of cases, storage, video, big data, security. In the storage case, they can be used to be used to accelerate storage apps and security, like encrypting data on the disk.
Using smartNICs with NVMeOF you can disagrate compute from storage and you can get local storage performance across the network, that is very close performance between local storage and remote storage. Basically you attach a block device to your VM and acessing it is as fast as if it was local.
NVMeOF maintains the high performance of a local nvme SSD. 

Many applications moving to cloud today are probably not in high demand of high speed access to disks, because they were written for HDDs to begin with. 
But in the future for sure 																																																																																																																																																																																																																																																																																																			
smartNICs with Storage - TO BE ADDED
Using smartNICs for NVMe termination
NVMe ov TCP , Roce v2
- advance to next slide - 
                                        </aside>
                                </section>
<!––  Slide13 ––>
                                <section> Ericsson customers are running on Openstack. 
                                        <aside class="notes">
List some operators, the olympics in Japan with docomo, etc - TO BE ADDED
- advance to next slide - 
                                        </aside>
                                </section>
<!––  Slide14 ––>
                                <section> Thank you part 1
                                        <aside class="notes">
 Before we take a break and grab those beers, kindly arranged by CN, I have two things to add.
1. I'd like to express my deepest gratitude to a person that inspires me quite a lot. Many of you know him, his name is FH and if you have never heard about this person, you're missing out... big time. I  attended one of his hands on workshops at OpenStack Nordics, by chance, then I found him on Twitter, then on YouTube, then I found his awesome blog. In the spirit of sharing my astounding discovery, I ran to my colleagues ”have y’all seen this guy's blog???” Some of them replied ” oh yeah, we've been reading his posts for years, he's really good!!” Argh, the kind of valuable information people don't share. Seriously, check out his talks on YouTube and his blog! 
- advance to next slide - 
                                        </aside>
                                </section>
<!––  Slide15 ––>
                                <section> Thank you part 2
                                        <aside class="notes">
2. Secondly, it's my desire to leave you with a thought. 
I came to the realization that most of the things we learn are provisional and in consequence they are open to recantation and refutation. I enjoy this path of questioning everything, why do we do things a certain way. 
In 2001 my first job was sysadmin working for a big Eastern European Internet Service Provider
Many customers asked to move their web servers and mail servers on our premises.
It was because their services would access directly the big pipe and we had a generator. 2001 Eastern European country meant many power outages,

They would bring desktop tower PCs we had a room with tables against the wall and we lined up the towers on these tables. Soon enough, we ran out of physical space on those tables and bought racks. We asked customers to buy rackable servers in order to host with us, then we ran out of space again and somewhere 2005 we started using VMs. 
Fast forward, today our datacenters are collections of those stacked boxes that used to be desktop computers. To me, this is insane, does it make any sense to you? Should we go down the timeline of computer history and have racks of disks and racks of memory and CPU and racks of NICs?
- advance to next slide -
                                        </aside>
                                </section>
<!––  SlideX ––>	
				<section> To DELETE
					<tr>
					    <th><img src="pics/slide8_intel-ericsson-logo.jpg" width=100% height=100%></th>
					</tr>
					<aside class="notes">
					At Ericsson,
					</aside>
				</section>
<!––  SlideX ––>	
                <section data-background-color="#FFC0CB">To DELETE
                                        <tr>
                                            <th><img src="pics/me.jpg" width=27% height=27%></th>
                                            <th><img src="pics/j.jpg" width=35% height=35%></th>
                                        </tr>
				</section>
                <section data-background-color="#FFC0CB">To DELETE
                                        <p>
					 DPDK/SPDK
                                        </p>
                </section>
                <section data-background-color="#FFC0CB" >To DELETE
                                        <p>
                                                <img src="pics/sn.png" width=75% height=75%>

                                        </p>    
               </section> 
               <section data-background-color="#FFC0CB" >To DELETE
                                        <p>
                                                <img src="pics/kernel-diag-ascii.svg" width=35% height=35%>
                                        </p>
               </section> 
               <section data-background-color="#FFC0CB">To DELETE
                                        <p>
                                           PCI bifurcation
                                        </p>
               </section>
               <section data-background-color="#FFC0CB">To DELETE
                                        <p>
                                                <img src="pics/termtosvg_5xxaxp7l.svg" width=60% height=60%>        
                                        </p>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>

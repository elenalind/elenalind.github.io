<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				
<!––  Slide0 ––>				
				<section><p><b>In the NIC of time</b></p>
				<p>Enabling high-performance edge applications with OpenStack, OVS, and SmartNICs<p/>
                                    <aside class="notes">
Thank you, Daniel, for the introduction.
<br><br>How many people are familiar with
<br>smartNICs
<br>XDP?
<br>BPF?
<br>Openstack?
                                    </aside>
				</section>
<!––  Slide1 ––>
				<section>
			        <img src="pics/slide1.png" width=70% height=70%></th>
				    <aside class="notes">
<p>So! Hello everyone, my name is Elena "I won't pronounce" Lindqvist and I am here to tell you about using smartNICs with OpenStack.</p>
<p>I work at Ericsson as a systems manager. That's *systems* manager, so ... </p>
				    </aside>
				</section>
<!––  Slide2 ––>				
				<section>
			        <img src="pics/slide2_meeting.jpg" width=70% height=70%></th>
				    <aside class="notes">
... not this ... 

				    </aside>
				</section>
<!––  Slide3 ––>	
				<section>
			        <img src="pics/slide3_HDS.jpg" width=70% height=70%></th>
					<aside class="notes">
but this ...
					</aside>
				</section>
<!––  Slide4 ––>	
				<section>
			        <img src="pics/slide3_datacenter.jpg" width=70% height=70%></th>
					<aside class="notes">
or this ...
					</aside>
				</section>		
<!––  Slide5 ––>			
				<section>
					<tr>
					<p>
					    <th>40% of all mobile traffic data</th>	
					</p>						
					</tr>
					<tr>
					    <th><img src="pics/slide5_erlang.jpg" width=25% height=25%></th> 
				        </tr>
					<aside class="notes">
If you don't mind, I'd like to tell you a few things about Ericsson. 
<br>We're NOT making phones! some people still think that we are ...
<br>I can tell you that no matter where you are in the world, when you are connected to a mobile network and you access the internet, there is a high chance that your traffic goes through our stuff, radio base stations, servers running ericsson software etc...
<br>40% of all mobile traffic world wide goes through our stuff.
<br>
<br>And since we are at an Openstack meetup and talking about open source, it is probably good to mention that Ericsson has given to open source Erlang (Ericsson Language, or named after a Danish mathematician, whichever you prefer)
<br>Erlang is widely used and in Openstack, RabbitMQ uses Erlang.
					</aside>
				</section>
<!––  Slide6 ––>	
				<section>
					<tr>
						<th><img src="pics/slide6_openstack-logo.png" width=34% height=34%></th> 
					</tr>
					<tr>
					    <th><img src="pics/slide6_nfv.jpeg" width=50% height=50%></th>
					</tr>

					<aside class="notes">
At Ericsson, we use OpenStack for virtual infrastructure management, as part of our NFVi solution. 
<br>Our customers are typically telecom operators ... well, and a taxi company ... in Dubai ... and Panasonic Avionics for entertainment on board of flights, and many other such examples. 
<br>
<br>What is NFVi? 
<br>For many years at Ericsson, we made SW and HW very dependent on each other. Customers bought whole racks of custom HW and the Ericsson SW applications deployed and running on top of that HW.
<br>NFVi is part of the NFV framework and it means, more or less, decoupling the SW from HW for network nodes using virtualization. 
<br>It means you can run the telecom applications(the SW) on any HW (like Dell, HP, Qanta, SuperMicro, Fujitsu servers, whatnot), in VMs or containers.
<br>
<br>Traces of this decoupling of the network functions from proprietary hardware appliances have been there for many years now. 
<br>Around 2003, I worked in an ISP. We used Cisco routers to do BGP with customers and the upstream provider. I was in awe when GNU Zebra came out and I could run BGP in a Linux box. 
<br>Fast forward to today, part of SDN, we use opendaylight with Quagga soft router for BGP ( Quagga is what followed after zebra, it is actually an extinct sub-specie of the African zebra.)
					</aside>
				</section>
<!––  SlideX ––>
                                <section> 									
<p><b>88%</b> increase in mobile data traffic  Q4-2017 to Q4-2018</p>
<p><b>EPC</b> to the rescue</p>										 
                                        <aside class="notes">                                      

<br>According to Ericsson Mobility Report, the monthly mobile data traffic increased to 88% between Q4 2017 and Q4 2018, this is mainly increasing traffic per smartphone in China. 
According to the same report, mobile traffic is 50% video today and it will increase to 75% video in 2020, driven by, amongst others, AR/VR applications. 
<br>
<br>EPC is handling all this traffic.
<br>EPC is the equivalent of formerly used GPRS, it is there to make mobile data traffic possible. 
<br>It means you traverse it when surfing the internet from your mobile, when watching youtube, Netflix, GoT, playing Pokemon?
<br>
<br>I work at E/// with infrastructure, so EPC people ask us for ways to solve this increasing traffic problem. How to cope with this amounts of traffic in a performant way?
<br>We think smartNIC can be the answer.                                    
								</aside>
                                </section>

<!––  Slide7 ––>	
				<section>
					<tr>
					    <th><img src="pics/slide7_smartNIC.png" width=100% height=100%></th>
					</tr>
					<aside class="notes">
This is how a smartNIC looks like!

<br>To cope with this increase in traffic, we are looking at using smartNICs in Openstack on top of which we run vEPC in VMs. 
<br>
<br>What is a smartNIC?
<br>A smartNIC is a NIC capable of offloading processing from the host CPU. To be able to do this, it can have its own CPU, memory and flash storage embedded on the card. 
<br>To offload processing from the CPU, a smartNIC will, for instance, run the ovs control and data plane, a firewall or performance acceleration, *inside* the NIC. 
<br>This means, you will run your ovs-vsctl show commands while logged in to the NIC
					</aside>
				</section>				
<!––  Slide8 ––>
                                <section>
                                        <tr>
                                            <th><img src="pics/slide8_intel.png" width=24% height=24%></th>
                                            <th><img src="pics/slide8_mellanox.jpg" width=45% height=45%></th>
                                        </tr>
                                        <tr>
                                            <th><img src="pics/slide8_Broadcom.jpg" width=44% height=44%></th>
                                            <th><img src="pics/slide8_netronome.png" width=45% height=45%></th>
                                        </tr>                                        
                                        <aside class="notes">
There are quite a few types of smartNICs, ranging from thousands to a few hundred dollars(some you can buy on eBay).
<br>Some smartNICs use an FPGA (Field Programmable Gate Array) mounted on the PCIe network card. Using FPGA boards provides flexibility, they can be easily programmed and updated once installed. (P4-Programmable Protocol-independent Packet Processor- can be used for programming packet forwarding planes.)
<br>
<br>Some smartNICs do not use an FPGA, and are ASIC based instead, they are less flexible but still capable of doing lots of things, at a significantly lower price. 
<br>
<br>SmartNICs might use ARM CPUs in a RISC architecure (the kind of HW you have on your *smart*phone) or x86 CPUs, different amounts of RAM, they can consume more or less power.
<br>
<br>SmartNICs can have different form factors HHHL(half hight half lenght) if you need to fit it in a 2Us compute  or FHFL full hight full lenght for wider than 2Us computes.
<br>
<br>You can opt for 2 or 4 X 25, 4X 50, 2 X 100 Gbps ports, that is two ports (cages) on a NIC wifh HHHL form factor and  4 ports on NICs with FHFL form factor. 
<br>You have options around the number of PCI lanes to be used, x8 or x16.  
                                        </aside>
                                </section>

<!––  Slide9 ––>
                                <section>
									                                        <tr>

					 <p>Is it a bird? is it a plane?</p>
					 <p> It's SmartNIC !</p>
					                                         </tr>

                                        <aside class="notes">
How should your architecture look like when using smartNICs.
<br>
<br>You could use one smartNIC for data per compute, we're talking 2x100 Gbps here, that's *a lot* of bandwidth.
In this case, high availability happens at compute level, not at network card level. 
<br><br>
What if you have a dual socket system. If you plug a smartNIC in a PCIe socket, applications might not like crossing that QPI link (from 2017 called UPI) between the CPUs.
In this case you can look into using a bifurcated smartNIC, that's basically splitting the card in two physical pieces that you can insert in two PCIe slots, one per NUMA node.
<br><br>
If you use two smartNICs, do you want two separate ovs controllers in your compute? will OpenStack even support that? 
                                        </aside>
                                </section>
                                
<!––  Slide9 ––>
                                <section>
                                        <tr>
					 <p>What is this smartNIC, anyway?</p>
					 <p>Is it an embedded linux? Is it a linux mini server?</p>					 
                                        </tr>
                                        <aside class="notes">

What is this smartNIC, anyway? Is it an embedded linux, is it a linux mini server?
<br>
<br>If you see it as embedded linux, then to upgrade the "FW" you would run an agent on the compute to do this. Is that good enough if you have many such smartNICs in your deployment?
<br>
<br>Would you want the possibility to say, in one go, configure all your smartNICs to PXE boot, so you can load a new linux with ovs patches on them?
<br>Then you need something like IPMI, hence it's something resembeling a linux server here than an embedded linux. 
<br>
<br>What security concerns are raised with introducing a smartNIC with linux running on it. I'm looking at you, Kim!
                                        </aside>
                                </section>                                
<!––  Slide9 ––>
                                <section>
                                        <tr>
					 <p>When should you use a smartNIC?</p>
					 <p>Do you use more than 4CPUs for OVS per compute host?</p>
					 
                                        </tr>
                                        <aside class="notes">

When should you use a smartNIC?
<br>(Intel info) If on your host you use more than 4CPUs for OVS, then you should switch to using smartNICs, it makes sense from a business point of view. 
<br>(Also, smartNIC is a good idea if you need low latency and don't care so much about migration)
                                        </aside>
                                </section>
<!––  SlideX ––>
                                <section> 
                                        <tr>
                                            <th><img src="pics/cpuisol1.png" width=90% height=90%></th>
                                        </tr>                                        
                                        <aside class="notes">                                      
Here comes an example where you can see the CPUs allocated to ovs under nohz_full ( confirmed with /etc/default/ovs-dpdk PMDs). If ovs is running on that CPU, the kernel will stop sending timer ticks to that CPU, running ovs rather than servicing interrupts and context switching. (but from my experience this works to a certain degree).
<br>
<br>Here's one interesting example: on a poor dimensioned compute (where you're running many VMs and did not take into account that you need to reserve resources for device emulation for each VM, *on the host OS side*), if you run out of mem , oom killer will take down your ovs and with it shutting down all your VMs? With ovs in the NIC this will not happen, but of course you still need to properly dimension your system!!
<br>
								</aside>
                                </section>

<!––  SlideX ––>
                                <section> 
                                        <tr>
                                            <th><img src="pics/cpuisol.png" width=99% height=99%></th>
                                        </tr>                                        
                                        <aside class="notes">                                      
Here comes another example for a high performance ovs compute, with more CPUs assigned to ovs.
								</aside>
                                </section>

                                                                                                
<!––  Slide10 ––>
                                <section>   <p>Ironic Neutron Cyborg</p>
                                        <tr>
                                            <th><img src="pics/OpenStack_Project_Ironic_mascot.png" width=25% height=25%></th>
                                            <th><img src="pics/OpenStack_Project_Neutron_mascot.png" width=25% height=25%></th>
                                            <th><img src="pics/OpenStack_Project_Cyborg.png" width=25% height=25%></th>
                                        </tr>
                                                                        
                                        <aside class="notes">
Now the question is where are we in Openstack when it comes to integrating the wide range of smartNICs popping up on the market?
<br>
<br>Work is done in several Openstack projects, like ironic, nova, neutron and cyborg.
<br>
<br>For instance when it comes to neutron, we need changes in the Neutron OVS driver and Neutron OVS agent in order to bind the Neutron port for the baremetal host with the smartNIC.
<br>This is needed so that neutron ovs agent can configure the OVS running on the smartNIC.
<br>
<br>We can have neutron ovs agent running locally on the smartNIC or remotely and manages the OVS bridges for all baremetal smartNICs. (well, good luck with that)
<br>
<br>
There are many interesting questions raised, like how do you know which smartNIC belongs to which server, in ironic we configure hostname and ssh keys...
<br><br>Openstack Cybborg works with providing managementAPIs to work with acceleration resources, like FPGA.

                                        </aside>
                                </section>

<!––  SlideX ––>
                                <section>
<font size="-2">										
<p style="text-align:left;">root@cic-1:~# openstack  port create<br>                                        
<p style="text-align:left;">usage: openstack port create [-h] [-f {json,shell,table,value,yaml}]<br>                                        
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[-c COLUMN] [--max-width <integer>] [--fit-width]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[--print-empty] [--noindent] [--prefix PREFIX]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--network <network> [--description <description>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--device <device-id>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--mac-address <mac-address>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--device-owner <device-owner>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--vnic-type <vnic-type>] [--host <host-id>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--dns-name dns-name]</p>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--fixed-ip subnet=<subnet>,ip-address=<ip-address>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--binding-profile <binding-profile>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--enable | --disable] [--project <project>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--project-domain <project-domain>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--security-group <security-group> | --no-security-group]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--qos-policy <qos-policy>]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--enable-port-security | --disable-port-security]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--allowed-address ip-address=<ip-address>[,mac-address=<mac-address>]]<br>
<p style="text-align:left;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; [--tag <tag> | --no-tag]<br>
<p style="text-align:left;">--vnic-type <vnic-type><br>
<p style="text-align:left;">VNIC type for this port (direct | direct-physical | macvtap | normal | baremetal, default: normal)</p>                         
                    
</font>									 
                                        <aside class="notes">                                      
Neutron ml2 OVS changes:

Introduce a new vnic_type for smart-nic.

Update the Neutron ml2 OVS to bind smart-nic vnic_type with binding:profile smart NIC config.Steps: 
(https://specs.openstack.org/openstack/neutron-specs/specs/stein/neutron-ovs-agent-support-baremetal-with-smart-nic.html)
1. Create Neutron port with smart-nic vnic_type , this is done in Neutron OVS ML2 driver 
2. local_link_information wiht info like smartNIC hostname , port ID, ssh public key, ovsdb ssl certificate                                    
								</aside>
                                </section>                                

<!––  SlideX ––>
                                <section>
<font size="-0.5">					 
<p style="text-align:left;">root@FPA1066GX-DA2:~# ovs-appctl  dpif/dump-flows br0</p>  
<p style="text-align:left;">in_port(14),eth(src=68:05:ca:91:36:b5,dst=68:05:ca:91:36:b5),eth_type(0x0800),ipv4(src=192.168.0.1,dst=192.168.1.1,proto=6),</p>                                        
<p style="text-align:left;">tcp(src=1234,dst=5678), packets:0, bytes:0, used:7.340s, actions:<b><font color="red">drop</font></b></p>  
</font>	                                                                       									
                                        <aside class="notes">                                      
<p>Yeah, I chucked this in just as an example of what it means to play with non-commercial NICs, can you spot the problem?</p>                                        
								</aside>
                                </section>
                                                                

<!––  Slide11 ––>
                                <section> 
<p>Performance</p>
<p>Kernel Space vs User Space</p>

                                        <aside class="notes">
											

If you care about latency and packet processing performance there are a few options. 

We basically need to overcome the limitations in the Linux kernel which is not ideal for *lots* of packet-processing.

Why is the Linux kernel a problem when we talk about latency and performance ( with performance, I mean higher throughput at a lower CPU cost)? 
Well, the Linux kernel is monolithic, it's millions of lines of code.
It contains lots of things like drivers (which makes the Linux kernel work with any hw, not just your specific hw/smartNIC), it allows running many applications at the same time by using a time sharing layer. Resources like CPU, mem exposed by the kernel are shared between all the processes running.

The networking stack inside the Linux kernel limits how many packets per second it can process, it was conceived to be slow, a decision taken 20 25 years ago, that packets should be delivered into sockets. If you want to be fast you dont do this upfront. 
Too many packets per second means CPUs get busy just receiving packets, then either the packets are dropped or we CPU starve the applications.



<!––  TAKE IT OUT this paragraph or rephrase it ––>
How your operating system deals with data
Data destined to a particular system is first received by the NIC and is stored in the ring buffer of the reception (RX) present in the NIC, which also has TX (for data transmission). Once the packet is accessible to the kernel, the device driver raises softirq (software interrupt), which makes the DMA (data memory access) of the system send that packet to the Linux kernel. The packet data in the Linux kernel gets stored in the sk_buff data structure to hold the packet up to MTU (maximum transfer unit). When all the packets are filled in the kernel buffer, they get sent to the upper processing layer – IP, TCP or UDP. The data then gets copied to the preferred data receiving process.
Note: Initially, a hard interrupt is raised by the device driver to send data to the kernel, but since this is an expensive task, it is replaced by a software interrupt. This is handled by the NAPI (new API), which makes the processing of incoming packets more efficient by putting the device driver in polling mode.
                                        </aside>
                                </section>


<!––  SlideX ––>
                                <section> 
<p>Kernel bypass with DPDK</p>                                  
                                        <aside class="notes">
To get better performance, one can choose to bypass the kernel, fully or partially.
There are several kernel bypass options like: 
DPDK, (that would be the poster child of kernel bypass)
Snabbswitch, 
PF_RING, 
Netmap. (I am personally more familiar with DPDK.)


With kernel bypass, we move the NIC to the user-space.
If the NIC is managed in the user-space, it means we skip things like context switching, networking layer processing, interrupts that happen in the kernel aka IRQ storms and do the packet-processing in the user-space.
This is relevant at 10Gbps already. EPC today saturates 200Gbps already. 

NUMA awareness together with CPU isolation needs to be considered as well if we need high performance. 
Remember this is particularly interesting when using smartNICs, with a dual socket system using one smartNIC plugged in on PCIe slot, corresponding to one NUMA socket.

Moving to userspace means losing the abstraction level the kernel provides for e.g. hw resources, it means you need to load own driver.
Moving to userspace means the kernel space is skipped together with the good stuff too like networking functionality that needs to be reimplemented now. (like the whole TCP stack)										                                      
                                       
								</aside>
                                </section>

<!––  SlideX ––>
                                <section> 

<font size="-2">										
<p style="text-align:left;">$ echo 0000:18:00.4 > /sys/bus/pci/devices/0000\:18\:00.4/driver/unbind<br>                                        
<p style="text-align:left;">$ echo 0000:18:00.3 > /sys/bus/pci/devices/0000\:18\:00.3/driver/unbind<br>
<p style="text-align:left;">$ echo 0000:18:00.4 > modprobe vfio<br>                                        
<p style="text-align:left;">$ echo 0000:18:00.3 > modprobe vfio_pci<br>
<br>  
<br>                                            
<p style="text-align:left;">$ mkdir -p /dev/hugepages/<br>
<p style="text-align:left;">$ mount -t  hugetlbfs hugetlbfs /dev/hugepages/<br>
<p style="text-align:left;">$ echo 2048 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages<br>
<br>  
<br>                                            
<p style="text-align:left;">$ modprobe uio<br>
<p style="text-align:left;">$ insmod x86_64-native-linuxapp-gcc/kmod/igb_uio.ko<br>
<p style="text-align:left;">$ dpdk-devbind.py -b igb_uio 18:00.2 18:00.3<br>

              
</font>	
                                  
                                        <aside class="notes">

How do you move a device from kernel space to user space, in case of DPDK?
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>XDP and eBPF</p>                                  
                                        <aside class="notes">  

Another way to achieve high performance would be partially bypassing the Linux kernel, for example using XDP. 
<br>  
<br>  
XDP  (eXpress Data Path) is a component in the kernel that can be used for fast packet processing. It is an eBPF (I'll get back to eBPF in a jiffy) based high performance data path merged in the Linux kernel.
<br>  
<br>  
XDP is shipped with the kernel since version 4.8 and it is enabled by default, with CONFIG_BPF_SYSCALL.										                                    
                                       
								</aside>
                                </section>
                                
<!––  SlideX ––>
                                <section> 
<font size="-1">									
<p style="text-align:left;">$ grep CONFIG_BPF_SYSCALL /boot/config-4.15.0-46-generic</p>                                                                                                                        
<p style="text-align:left;">CONFIG_BPF_SYSCALL=y</p> 
<br>  
<br>
<p style="text-align:left;">prompt: XDP sockets</p> 
<p style="text-align:left;">type: bool</p> 
<p style="text-align:left;">depends on: CONFIG_BPF_SYSCALL</p> 
<p style="text-align:left;">defined in net/xdp/Kconfig</p> 
<p style="text-align:left;">found in Linux kernels: 4.18–4.20, 5.0–5.1, 5.2-rc+HEAD</p> 
                                                                                                                                                    
</font>	
                                        <aside class="notes"> 										
To check if XDP is enabled in the kernel, it's as simple as grepping for it in the kernel config file 
<br>  
<br>  
The Linux kernel configuration item CONFIG_XDP_SOCKETS:       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
                                        <p>
                                                <img src="pics/kernel-diag-ascii.svg" width=35% height=35%>
                                        </p>                                 
                                        <aside class="notes"> 

So XDP is a hook in the Linux kernel, not a kernel bypass but a bypass of the network stack in the linux kernel. It is a programmable layer in the kernel network stack.
<br>  
XDP operates at the same level as DPDK, directly on the packet buffers, operating on the packet before it is moved to socket buffer. 
The network stack in the kernel is conceived to be slow. There was a decision taken 20 or 25 years go, that packets should be delivered into sockets. 
This is not the fastest way, maybe you don't need to deliver all packets to sockets, think if you need to drop some of these packets for instance. 
<br>  
<br>
While DPDK steals the whole NIC from linux kernel, XDP does not still the NIC from the kernel. In XDP we put in a filter per receive queue, make decisions on the packets with zero copy to userspace. This way all the fun features in kernel are available. 
<br>  
<br>
XDP can be used in two modes: native mode, generic (SKB) mode
<br>  
Native mode XDP (process packet right after DMA, just before SKB allocation), a driver hook , before memory allocation , small no of instructions executed before we start processing packets
<br>  
Limited because we need support for XDP in the driver, but higher performance mode.
<br>  
<br>
Generic mode
<br>  
IT happens after DMA and SKB allocation, it means a larger number of instructions is executed before doing something with that packet, so this means significantly lower performance than native mode.
Works on any NIC device, driver independent. 
<br>  
<br>  
You can look at XDP and BPF with a dataplane/controlplane approach.

<br>  
The dataplane is in the kernel and the control plane is in the userspace, where userspace load eBPF program.
<br>  
<br>  
(spanning tree daemon uses BPF to filter only BPDUs are comming on that socket)						                                     



                                       
								</aside>
                                </section>     
<!––  SlideX ––>
                                <section> 
<p>XDP_DROP</p>
<p>XDP_PASS</p>
<p>XDP_TX</p>
<p>XDP_ABORTED</p>
<p>XDP_REDIRECT</p>

                                        <aside class="notes">                                      
Thse are actions that can be executed on a packet
<br><br> DROP is awesome, useful for DDOS, fb uses it heavily and cloudflare
<br><br> PASS it lets the packet pass, after packet inspection, this does not do DPDK , light weight as you program it , lets the packet go back to stack 
<br><br> TX send it immedietly back out on the port that it was received, for load balancer cases for example 
<br><br> ABORTED basically drop but what is extra is that you will get some log about it , useful for debugging for sysadmin or developer
<br><br> REDIRECT to another port, to other CPUs, you can modify headers on the packet (TX and REDIRECT similar to the DPDK ones)
<br><br>  no support for jumbo frames in XDP                                       
								</aside>
                                </section>
                                                                                           
<!––  SlideX ––>
                                <section> 
<p>eBPF is a superpower</p>                                  
                                        <aside class="notes">                                      
What the heck is eBPF ?
<br><br>BPF stands for  "Berkeley Packet Filter" it's a linux kernel technology that is used by e.g. tcpdump and other analysis tools.
<br><br>eBPF is used to extract millions of metrics from the kernel and applications for troubleshooting purposes, deep monitoring or exploring running software.
<br><br>BPF is basically a superpower.
<br><br>BPF was initially used for tools like tcpdump but Alexei Starovoitov introduced eBPF (enchanced BPF) to be used  for things like NATing, routing, doing what iptables does for example.  			                                       
								</aside>
                                </section>

<!––  SlideX ––>
                                <section> 
                                        <tr>
                                            <th><img src="pics/bpf2.png" width=99% height=99%></th>
                                        </tr>                                    
                                        <aside class="notes">                                      
This assembly instructions set, this is BPF.                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
<!––  SlideX ––>
                                <section> 
<p>SlideX</p>                                  
                                        <aside class="notes">                                      
                                       
								</aside>
                                </section>
                                 


<!––  Slide12 ––>
                                <section> smartNICs with Storage
                                        <aside class="notes">
<br>I talked mostly about the data traffic so far and using smartNICs with the data traffic.
<br><br>smartNICs make sense in a lot of cases, storage, video, big data, security. In the storage case, they can be used to be used to accelerate storage apps and security, like encrypting data on the disk.
<br>Using smartNICs with NVMeOF you can disagrate compute from storage and you can get local storage performance across the network, that is very close performance between local storage and remote storage. 
<br>Basically you attach a block device to your VM and acessing it is as fast as if it was local.
<br>NVMeOF maintains the high performance of a local nvme SSD. 
<br><br>Many applications moving to cloud today are probably not in high demand of high number of IOPS, because they were written for HDDs to begin with. 
<br>But smartNICs with storage will be relevant in the  future.
																																																																						
                                        </aside>
                                </section>
<!––  Slide13 ––>
                                <section> Ericsson customers that are running on Openstack. 
                                        <aside class="notes">
List some operators, the olympics in Japan with docomo, etc - TO BE ADDED
- advance to next slide - 
                                        </aside>
                                </section>
<!––  Slide14 ––>
                                <section>
                                        <tr>
                                            <th><img src="pics/fh.png" width=99% height=99%></th>
                                        </tr>                                
                                        <aside class="notes">
 Before we take a break and grab those beers, kindly arranged by CN, I have two things to add.
1. I'd like to express my deepest gratitude to a person that inspires me quite a lot. Many of you know him, his name is FH and if you have never heard about this person, you're missing out... big time. I  attended one of his hands on workshops at OpenStack Nordics, by chance, then I found him on Twitter, then on YouTube, then I found his awesome blog. In the spirit of sharing my astounding discovery, I ran to my colleagues ”have y’all seen this guy's blog???” Some of them replied ” oh yeah, we've been reading his posts for years, he's really good!!” Argh, the kind of valuable information people don't share. Seriously, check out his talks on YouTube and his blog! 
- advance to next slide - 
                                        </aside>
                                </section>
<!––  Slide15 ––>
                                <section> 
<br>Datacenters today?
<br>Stacked up "desktop" computers filled with ... air? 
                                        <aside class="notes">
<br>Secondly, it's my desire to leave you with a thought. 
<br>I came to the realization that most of the things we learn are provisional and in consequence they are open to recantation and refutation. I enjoy this path of questioning everything, why do we do things a certain way. 
<br><br>In 2001 my first job was sysadmin working for a big Eastern European Internet Service Provider
Many customers asked to move their web servers and mail servers on our premises.
It was because their services would access directly the big pipe and we had a generator. 2001 Eastern Europe meant lots of power outages ... daily.

<br><br>They would bring desktop tower PCs we had a room with tables against the wall and we lined up the towers on these tables. Soon enough, we ran out of physical space on those tables and bought racks. We asked customers to buy rackable servers in order to host with us, then we ran out of space again and somewhere 2005 we started using VMs. 
Fast forward, today our datacenters are collections of those stacked boxes that used to be desktop computers. To me, this is insane, does it make any sense to you? Should we go down the timeline of computer history and have racks of disks and racks of memory and CPU and racks of NICs?
- advance to next slide -
                                        </aside>
                                </section>


			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
